{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4065a96-3470-4760-a83d-4972ed8cb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Centralizando dados dos participantes...\n",
      "[Info] Dados centralizados com sucesso. Total de registros: 864000!\n",
      "    dataReceived      timestamp      ppg  status participant_id\n",
      "0  1711437352342  1711437340286  2097152       0            P11\n",
      "1  1711437352342  1711437340327  2097152       0            P11\n",
      "2  1711437352342  1711437340369  2462981       0            P11\n",
      "3  1711437352342  1711437340408  2462958       0            P11\n",
      "4  1711437352342  1711437340447  2462983       0            P11\n",
      "\n",
      "[Info] Aplicando filtro passa-baixa Butterworth para remover ruído de alta frequência dos sinais PPG...\n",
      "\n",
      "[Info] Extraindo features HRV...\n",
      "\n",
      "[Info] Definindo rótulo de anormalidade cardíaca...\n",
      "\n",
      "[Info] Normalizando as features...\n",
      "\n",
      "[Info] Separando dados em treino (60%) e teste (40%)...\n",
      "\n",
      "[Info] Aplicando SMOTE apenas no conjunto de treino...\n",
      "[Info] Após SMOTE - Classe 0: 1487, Classe 1: 1487\n",
      "\n",
      "[Info] Treinando modelo com 2974 exemplos...\n",
      "\n",
      "[Nuvem] Salvando modelo final...\n",
      "[Nuvem] Modelo salvo em 'Modelo_Centralizado_MLP.json'.\n",
      "\n",
      "[Info] Avaliando modelo com dados de treino...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1487\n",
      "           1       0.99      1.00      0.99      1487\n",
      "\n",
      "    accuracy                           0.99      2974\n",
      "   macro avg       0.99      0.99      0.99      2974\n",
      "weighted avg       0.99      0.99      0.99      2974\n",
      "\n",
      "Acurácia final do modelo: 0.9950\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1477   10]\n",
      " [   5 1482]]\n",
      "\n",
      "[Info] Avaliando modelo com dados de teste...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       992\n",
      "           1       0.99      0.99      0.99       390\n",
      "\n",
      "    accuracy                           0.99      1382\n",
      "   macro avg       0.99      0.99      0.99      1382\n",
      "weighted avg       0.99      0.99      0.99      1382\n",
      "\n",
      "Acurácia final do modelo: 0.9949\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[987   5]\n",
      " [  2 388]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregamento dos dados PPG\n",
    "print(f\"\\n[Info] Centralizando dados dos participantes...\")\n",
    "base_path = r'C:\\Users\\ferna\\Documents\\Unisinos\\Cadeiras\\TCC 2\\Dataset utilizado\\GalaxyPPG\\Dataset'\n",
    "\n",
    "# Listar manual com os IDs dos participantes escolhidos\n",
    "selected_ids = ['P11', 'P12', 'P13', 'P14', 'P15', 'P17', 'P18', 'P19', 'P20']\n",
    "\n",
    "# Listar para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre os IDs e carrega cada arquivo\n",
    "for participant_id in selected_ids:\n",
    "    ppg_path = os.path.join(base_path, participant_id, 'GalaxyWatch', 'PPG.csv')\n",
    "    \n",
    "    if os.path.exists(ppg_path):\n",
    "        df = pd.read_csv(ppg_path)\n",
    "        df['participant_id'] = participant_id  # Adiciona a origem dos dados\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"[Aviso] Arquivo não encontrado: {ppg_path}\")\n",
    "\n",
    "# Concatenar todos os dados em um único DataFrame\n",
    "df_centralizado = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Exibir resultado\n",
    "print(f\"[Info] Dados centralizados com sucesso. Total de registros: {len(df_centralizado)}!\")\n",
    "print(df_centralizado.head())\n",
    "\n",
    "ppg_signal = df_centralizado['ppg'].values     # Extrai o sinal PPG (fotopletismografia)\n",
    "ppg_ts = df_centralizado['timestamp'].values   # Extrai os timestamps (não usado diretamente)\n",
    "fs = 25  # Frequência de amostragem do sinal (25 Hz)\n",
    "\n",
    "# Filtragem do sinal PPG\n",
    "print(f\"\\n[Info] Aplicando filtro passa-baixa Butterworth para remover ruído de alta frequência dos sinais PPG...\")\n",
    "\n",
    "def butter_lowpass(data, cutoff=5, fs=25, order=3):\n",
    "    nyq = 0.5 * fs  # Frequência de Nyquist\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = signal.filtfilt(b, a, data)  # Filtro passa-baixa aplicado em modo zero-phase (sem atraso)\n",
    "    return y\n",
    "\n",
    "ppg_filtered = butter_lowpass(ppg_signal, cutoff=5, fs=fs, order=3)\n",
    "\n",
    "# Função para extrair features HRV a partir dos picos detectados\n",
    "def extract_hrv_features_from_peaks(peaks, fs):\n",
    "    try:\n",
    "        if len(peaks) < 3:\n",
    "            raise ValueError(\"Poucos picos detectados para análise HRV\")\n",
    "\n",
    "        ibis = np.diff(peaks) / fs\n",
    "        ibis_ms = ibis * 1000\n",
    "\n",
    "        rmssd = np.sqrt(np.mean(np.square(np.diff(ibis_ms))))\n",
    "        sdnn = np.std(ibis_ms)\n",
    "        pnn50 = np.sum(np.abs(np.diff(ibis_ms)) > 50) / len(ibis_ms)\n",
    "        mean_ibi = np.mean(ibis_ms)\n",
    "        iqr_ibi = np.percentile(ibis_ms, 75) - np.percentile(ibis_ms, 25)\n",
    "\n",
    "        return [rmssd, sdnn, pnn50, mean_ibi, iqr_ibi]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no cálculo HRV: {e}\")\n",
    "        return [np.nan] * 5\n",
    "\n",
    "# Extração das features com janela móvel\n",
    "window_size = 30 * fs\n",
    "step_size = 10 * fs\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(f\"\\n[Info] Extraindo features HRV...\")\n",
    "\n",
    "print(f\"\\n[Info] Definindo rótulo de anormalidade cardíaca...\")\n",
    "\n",
    "for start in range(0, len(ppg_filtered) - window_size, step_size):\n",
    "    segment = ppg_filtered[start:start + window_size]\n",
    "    peaks, _ = find_peaks(segment, distance=int(fs * 0.5))\n",
    "    if len(peaks) < 3:\n",
    "        continue\n",
    "    hrv = extract_hrv_features_from_peaks(peaks, fs)\n",
    "    if np.any(np.isnan(hrv)):\n",
    "        continue\n",
    "    rmssd, sdnn = hrv[0], hrv[1]\n",
    "    label = int((rmssd < 20) or (sdnn < 50))\n",
    "    features.append(hrv)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Normalização e divisão treino-validação-teste\n",
    "print(f\"\\n[Info] Normalizando as features...\")\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\n[Info] Separando dados em treino (60%) e teste (40%)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\n[Info] Aplicando SMOTE apenas no conjunto de treino...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"[Info] Após SMOTE - Classe 0: {sum(y_train == 0)}, Classe 1: {sum(y_train == 1)}\")\n",
    "\n",
    "# Treinamento do modelo MLP com partial_fit\n",
    "CLASSES = np.array([0, 1])\n",
    "\n",
    "# Criar modelo MLP\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(200, 100, 50, 25), # Tupla definindo o número de neurônios em cada camada oculta. Aqui, 4 camadas ocultas com tamanhos variados\n",
    "    activation='tanh',                     # Função de ativação usada nos neurônios \n",
    "    solver='adam',                         # Algoritmo de otimização usado para ajustar pesos (adam = Estimativa Adaptativa de Momentos).\n",
    "    learning_rate_init=0.005,              # Taxa de aprendizado inicial para o otimizador (quanto maior, mais rápido o ajuste, mas pode ser instável).\n",
    "    alpha=0.001,                           # Parâmetro de regularização L2 para evitar overfitting (penaliza pesos grandes).\n",
    "    max_iter=1000,                         # Número máximo de iterações (epochs) por chamada de fit.\n",
    "    warm_start=True,                       # Permite continuar o treinamento a partir do estado anterior (importante para treino incremental).\n",
    "    n_iter_no_change=10,                   # Número de iterações sem melhora para parar o treino (não muito usado aqui, pois max_iter=1).\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinamento\n",
    "print(f\"\\n[Info] Treinando modelo com {len(X_train)} exemplos...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Salvar modelo final em arquivo JSON\n",
    "MODEL_FILE = 'Modelo_Centralizado_MLP.json'\n",
    "print(f\"\\n[Nuvem] Salvando modelo final...\")\n",
    "final_model_data = {\n",
    "    \"class\": model.__class__.__name__,\n",
    "    \"params\": model.get_params(),\n",
    "    \"coef\": [layer.tolist() for layer in model.coefs_],\n",
    "    \"intercept\": [b.tolist() for b in model.intercepts_]\n",
    "}\n",
    "with open(MODEL_FILE, 'w') as f:\n",
    "    json.dump(final_model_data, f)\n",
    "print(f\"[Nuvem] Modelo salvo em '{MODEL_FILE}'.\")\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(f\"\\n[Info] Avaliando modelo com dados de treino...\")\n",
    "y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "y_pred = (y_train_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(f\"Acurácia final do modelo: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "print(f\"\\n[Info] Avaliando modelo com dados de teste...\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_test_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Acurácia final do modelo: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea33dc-bdf5-494b-a0ec-1dcc4c36d871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
