{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4065a96-3470-4760-a83d-4972ed8cb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Centralizando dados dos participantes...\n",
      "[Info] Dados centralizados com sucesso. Total de registros: 864000!\n",
      "    dataReceived      timestamp      ppg  status participant_id\n",
      "0  1711437352342  1711437340286  2097152       0            P11\n",
      "1  1711437352342  1711437340327  2097152       0            P11\n",
      "2  1711437352342  1711437340369  2462981       0            P11\n",
      "3  1711437352342  1711437340408  2462958       0            P11\n",
      "4  1711437352342  1711437340447  2462983       0            P11\n",
      "\n",
      "[Info] Aplicando filtro passa-baixa Butterworth para remover ruído de alta frequência dos sinais PPG...\n",
      "\n",
      "[Info] Extraindo features HRV...\n",
      "\n",
      "[Info] Definindo rótulo de anormalidade cardíaca...\n",
      "\n",
      "[Info] Normalizando as features...\n",
      "\n",
      "[Info] Separando dados em treino (60%) e teste (40%)...\n",
      "\n",
      "[Info] Aplicando SMOTE apenas no conjunto de treino...\n",
      "[Info] Após SMOTE - Classe 0: 1487, Classe 1: 1487\n",
      "\n",
      "[Info] Treinando modelo com 2974 exemplos...\n",
      "\n",
      "[Nuvem] Salvando modelo final...\n",
      "[Nuvem] Modelo salvo em 'Modelo_Centralizado_LR.json'.\n",
      "\n",
      "[Info] Avaliando modelo com dados de treino...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      1487\n",
      "           1       0.96      1.00      0.98      1487\n",
      "\n",
      "    accuracy                           0.98      2974\n",
      "   macro avg       0.98      0.98      0.98      2974\n",
      "weighted avg       0.98      0.98      0.98      2974\n",
      "\n",
      "Acurácia final do modelo: 0.9778\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1422   65]\n",
      " [   1 1486]]\n",
      "\n",
      "[Info] Avaliando modelo com dados de teste...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       992\n",
      "           1       0.88      1.00      0.93       390\n",
      "\n",
      "    accuracy                           0.96      1382\n",
      "   macro avg       0.94      0.97      0.95      1382\n",
      "weighted avg       0.97      0.96      0.96      1382\n",
      "\n",
      "Acurácia final do modelo: 0.9602\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[937  55]\n",
      " [  0 390]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregamento dos dados PPG\n",
    "print(f\"\\n[Info] Centralizando dados dos participantes...\")\n",
    "base_path = r'C:\\Users\\ferna\\Documents\\Unisinos\\Cadeiras\\TCC 2\\Dataset utilizado\\GalaxyPPG\\Dataset'\n",
    "selected_ids = ['P11', 'P12', 'P13', 'P14', 'P15', 'P17', 'P18', 'P19', 'P20']\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre os IDs e carrega cada arquivo\n",
    "for participant_id in selected_ids:\n",
    "    ppg_path = os.path.join(base_path, participant_id, 'GalaxyWatch', 'PPG.csv')\n",
    "    \n",
    "    if os.path.exists(ppg_path):\n",
    "        df = pd.read_csv(ppg_path)\n",
    "        df['participant_id'] = participant_id  # Adiciona a origem dos dados\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f\"[Aviso] Arquivo não encontrado: {ppg_path}\")\n",
    "\n",
    "# Concatenar todos os dados em um único DataFrame\n",
    "df_centralizado = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(f\"[Info] Dados centralizados com sucesso. Total de registros: {len(df_centralizado)}!\")\n",
    "print(df_centralizado.head())\n",
    "\n",
    "ppg_signal = df_centralizado['ppg'].values     # Extrai o sinal PPG (fotopletismografia)\n",
    "ppg_ts = df_centralizado['timestamp'].values   # Extrai os timestamps (não usado diretamente)\n",
    "fs = 25  # Frequência de amostragem do sinal (25 Hz)\n",
    "\n",
    "# Filtragem do sinal PPG\n",
    "print(f\"\\n[Info] Aplicando filtro passa-baixa Butterworth para remover ruído de alta frequência dos sinais PPG...\")\n",
    "\n",
    "def butter_lowpass(data, cutoff=5, fs=25, order=3):\n",
    "    nyq = 0.5 * fs  # Frequência de Nyquist\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = signal.filtfilt(b, a, data)  # Filtro passa-baixa aplicado em modo zero-phase (sem atraso)\n",
    "    return y\n",
    "\n",
    "ppg_filtered = butter_lowpass(ppg_signal, cutoff=5, fs=fs, order=3)\n",
    "\n",
    "# Função para extrair features HRV a partir dos picos detectados\n",
    "def extract_hrv_features_from_peaks(peaks, fs):\n",
    "    try:\n",
    "        if len(peaks) < 3:\n",
    "            raise ValueError(\"Poucos picos detectados para análise HRV\")\n",
    "\n",
    "        # Calcula os intervalos entre batimentos (IBI) em segundos\n",
    "        ibis = np.diff(peaks) / fs\n",
    "        ibis_ms = ibis * 1000  # Converte para milissegundos\n",
    "\n",
    "        # Cálculo das métricas HRV\n",
    "        rmssd = np.sqrt(np.mean(np.square(np.diff(ibis_ms))))       # Raiz da média dos quadrados das diferenças consecutivas\n",
    "        sdnn = np.std(ibis_ms)                                       # Desvio padrão dos intervalos\n",
    "        pnn50 = np.sum(np.abs(np.diff(ibis_ms)) > 50) / len(ibis_ms) # Proporção de diferenças maiores que 50ms\n",
    "        mean_ibi = np.mean(ibis_ms)                                  # Média dos intervalos\n",
    "        iqr_ibi = np.percentile(ibis_ms, 75) - np.percentile(ibis_ms, 25)  # Intervalo interquartil\n",
    "\n",
    "        return [rmssd, sdnn, pnn50, mean_ibi, iqr_ibi]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no cálculo HRV: {e}\")\n",
    "        return [np.nan] * 5\n",
    "\n",
    "# Extração das features com janela móvel\n",
    "window_size = 30 * fs  # Janela de 30 segundos\n",
    "step_size = 10 * fs    # Passo de 10 segundos (sobreposição de 20 segundos)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(f\"\\n[Info] Extraindo features HRV...\")\n",
    "\n",
    "print(f\"\\n[Info] Definindo rótulo de anormalidade cardíaca...\")\n",
    "\n",
    "for start in range(0, len(ppg_filtered) - window_size, step_size):\n",
    "    segment = ppg_filtered[start:start + window_size]\n",
    "\n",
    "    # Detectar picos no segmento com distância mínima de 0.5s entre eles (ajustável)\n",
    "    peaks, _ = find_peaks(segment, distance=int(fs * 0.5))\n",
    "\n",
    "    # Ignorar segmentos com poucos picos detectados (não confiável para análise)\n",
    "    if len(peaks) < 3:\n",
    "        continue\n",
    "\n",
    "    # Extrair as métricas HRV a partir dos picos detectados\n",
    "    hrv = extract_hrv_features_from_peaks(peaks, fs)\n",
    "\n",
    "    # Ignorar segmentos com métricas inválidas (NaN)\n",
    "    if np.any(np.isnan(hrv)):\n",
    "        continue\n",
    "\n",
    "    rmssd, sdnn = hrv[0], hrv[1]\n",
    "\n",
    "    # Definir o label binário (0 = normal, 1 = possível anormalidade)\n",
    "    label = int((rmssd < 20) or (sdnn < 50))\n",
    "\n",
    "    features.append(hrv)\n",
    "    labels.append(label)\n",
    "\n",
    "# Converter listas para arrays NumPy\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Normalização e divisão treino-validação-teste\n",
    "print(f\"\\n[Info] Normalizando as features...\")\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)  # Normaliza as features para média 0 e desvio padrão 1\n",
    "\n",
    "print(f\"\\n[Info] Separando dados em treino (60%) e teste (40%)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\n[Info] Aplicando SMOTE apenas no conjunto de treino...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"[Info] Após SMOTE - Classe 0: {sum(y_train == 0)}, Classe 1: {sum(y_train == 1)}\")\n",
    "\n",
    "# Treinamento do modelo de Regressão Logística\n",
    "# Parâmetros gerais\n",
    "CLASSES = np.array([0, 1])\n",
    "\n",
    "# Criar modelo\n",
    "model = SGDClassifier(\n",
    "    loss='log_loss',         # Usa regressão logística (função de perda logarítmica) para classificação binária\n",
    "    penalty='l1',            # Regularização L1 para gerar modelos esparsos e evitar overfitting\n",
    "    alpha=0.0005,            # Força da regularização (moderada), controla penalidade para evitar overfitting\n",
    "    max_iter=1000,           # Apenas uma iteração por chamada, ideal para treino incremental / federado\n",
    "    learning_rate='optimal', # Ajusta automaticamente a taxa de aprendizado para melhor convergência\n",
    "    eta0=0.1,                # Taxa de aprendizado inicial relativamente alta para acelerar o treino\n",
    "    shuffle=True,            # Embaralha os dados a cada iteração para melhorar a generalização\n",
    "    class_weight=None,       # Sem balanceamento automático, pois o SMOTE já balanceia os dados\n",
    "    average=False,           # Não usa média móvel dos coeficientes; treina diretamente os parâmetros atuais\n",
    "    random_state=42          # Garante reprodutibilidade dos resultados\n",
    ")\n",
    "\n",
    "# Treinamento\n",
    "print(f\"\\n[Info] Treinando modelo com {len(X_train)} exemplos...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Salvar modelo final em arquivo JSON\n",
    "MODEL_FILE = 'Modelo_Centralizado_LR.json'\n",
    "print(f\"\\n[Nuvem] Salvando modelo final...\")\n",
    "final_model_data = {\n",
    "    \"class\": model.__class__.__name__,\n",
    "    \"params\": model.get_params(),\n",
    "    \"coef\": model.coef_.tolist(),\n",
    "    \"intercept\": model.intercept_.tolist()\n",
    "}\n",
    "with open(MODEL_FILE, 'w') as f:\n",
    "    json.dump(final_model_data, f)\n",
    "print(f\"[Nuvem] Modelo salvo em '{MODEL_FILE}'.\")\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(f\"\\n[Info] Avaliando modelo com dados de treino...\")\n",
    "y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "y_pred = (y_train_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(f\"Acurácia final do modelo: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "print(f\"\\n[Info] Avaliando modelo com dados de teste...\")\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_test_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Acurácia final do modelo: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afca51-bfe1-4c1f-b7e7-37903be588e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
